{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from GCNFrame import Biodata\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example to train a two-classes model.\n",
    "from GCNFrame import Biodata, GCNmodel\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = Biodata(fasta_file=\"/root/autodl-tmp/TaxoChallenge/ICVTTaxoChallenge_43587.fa\", \n",
    "        label_file=\"/root/autodl-tmp/TaxoChallenge/phylum_numeric_labels.txt\",\n",
    "        feature_file=None)\n",
    "dataset = data.encode(thread=20)\n",
    "model = GCNmodel.model(label_num=20, other_feature_dim=0).to(device)\n",
    "GCNmodel.train(dataset, model, weighted_sampling=True, batch_size=16, model_name=\"/root/autodl-tmp/TaxoChallenge/GCN_model_43587.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the format of your phylum label file is one label per line\n",
    "def prepare_phylum_labels(label_file):\n",
    "    # Read the label file\n",
    "    labels_df = pd.read_csv(label_file, sep='\\t', header=None, \n",
    "                           names=['sequence_id', 'phylum'])\n",
    "    \n",
    "    # Convert text labels to numeric labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    numeric_labels = label_encoder.fit_transform(labels_df['phylum'])\n",
    "    \n",
    "    # Save the label mapping for interpreting results later\n",
    "    label_mapping = dict(zip(label_encoder.classes_, \n",
    "                           range(len(label_encoder.classes_))))\n",
    "    \n",
    "    # Save the numeric labels in numpy array format\n",
    "    np.savetxt('/root/autodl-tmp/TaxoChallenge/phylum_numeric_labels.txt', numeric_labels, fmt='%d')\n",
    "    \n",
    "    # Save the label mapping relationship\n",
    "    with open('/root/autodl-tmp/TaxoChallenge/phylum_label_mapping.txt', 'w') as f:\n",
    "        for phylum, idx in label_mapping.items():\n",
    "            f.write(f\"{phylum}\\t{idx}\\n\")\n",
    "    \n",
    "    return numeric_labels, label_mapping\n",
    "\n",
    "# Call the function\n",
    "prepare_phylum_labels(\"/root/autodl-tmp/TaxoChallenge/ICTV_TaxoChallenge_id_phylum.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BipartiteData class\n",
    "class BipartiteData(Data):\n",
    "    def _add_other_feature(self, other_feature):\n",
    "        self.other_feature = other_feature\n",
    "\n",
    "    def __inc__(self, key, value):\n",
    "        if key == 'edge_index':\n",
    "            return torch.tensor([[self.x_src.size(0)], [self.x_dst.size(0)]])\n",
    "        else:\n",
    "            return super(BipartiteData, self).__inc__(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_embeddings_from_model(model, data, device):\n",
    "    \"\"\"Extract matrix embeddings from the model, and return labels as well\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move data to the specified device\n",
    "        x_f = data.x_src.to(device)\n",
    "        x_p = data.x_dst.to(device)\n",
    "        edge_index_forward = data.edge_index[:, ::2].to(device)\n",
    "        edge_index_backward = data.edge_index[[1, 0], :][:, 1::2].to(device)\n",
    "\n",
    "        # Process primary node features\n",
    "        if model.pnode_nn:\n",
    "            x_p = torch.reshape(x_p, (-1, model.pnode_num * model.pnode_dim))\n",
    "            x_p = model.pnode_d(x_p)\n",
    "            x_p = torch.reshape(x_p, (-1, model.node_hidden_dim))\n",
    "        else:\n",
    "            x_p = torch.reshape(x_p, (-1, model.pnode_dim))\n",
    "\n",
    "        # Process feature node features\n",
    "        if model.fnode_nn:\n",
    "            x_f = torch.reshape(x_f, (-1, model.fnode_num))\n",
    "            x_f = model.fnode_d(x_f)\n",
    "            x_f = torch.reshape(x_f, (-1, model.node_hidden_dim))\n",
    "        else:\n",
    "            x_f = torch.reshape(x_f, (-1, 1))\n",
    "\n",
    "        # Add label embeddings (if supported)\n",
    "        if hasattr(model, 'label_embedding') and hasattr(data, 'y'):\n",
    "            label_embedding = model.label_embedding(data.y)\n",
    "            x_p = x_p + label_embedding.unsqueeze(1).expand(-1, x_p.size(1), -1)\n",
    "\n",
    "        # Graph convolution layers\n",
    "        for i in range(model.gcn_layer_num):\n",
    "            x_p = model.gconvs_1[i]((x_f, x_p), edge_index_forward)\n",
    "            x_p = F.relu(x_p)\n",
    "            x_f = model.gconvs_2[i]((x_p, x_f), edge_index_backward)\n",
    "            x_f = F.relu(x_f)\n",
    "            if not i == model.gcn_layer_num - 1:\n",
    "                x_p = model.lns[i](x_p)\n",
    "                x_f = model.lns[i](x_f)\n",
    "\n",
    "        # Convert to matrix form\n",
    "        x_p = torch.reshape(x_p, (-1, model.gcn_dim, model.pnode_num))\n",
    "        for i in range(model.cnn_layer_num):\n",
    "            x_p = model.convs[i](x_p)\n",
    "            x_p = F.relu(x_p)\n",
    "\n",
    "        # Return matrix form and labels (if available)\n",
    "        return x_p.cpu(), data.y.cpu() if hasattr(data, 'y') else None\n",
    "\n",
    "\n",
    "def get_matrix_dataset_embeddings(dataset, model_path, fasta_ids, batch_size=8, device=None):\n",
    "    \"\"\"Get matrix-form embeddings of the entire dataset, store them as a dictionary with fasta sequence IDs\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Processing dataset with {len(dataset)} samples...\")\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        follow_batch=['x_src', 'x_dst'])\n",
    "\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fasta_id_index = 0  # Track the index of fasta_ids\n",
    "\n",
    "        for batch in tqdm(loader, desc=\"Processing batches\"):\n",
    "            try:\n",
    "                # Extract matrix embeddings and labels\n",
    "                embedding, label = get_matrix_embeddings_from_model(model, batch, device)\n",
    "\n",
    "                # Use fasta_ids as keys to store embeddings in the dictionary\n",
    "                for idx in range(embedding.shape[0]):\n",
    "                    contig_id = fasta_ids[fasta_id_index]\n",
    "                    embeddings_dict[contig_id] = embedding[idx].numpy()\n",
    "                    fasta_id_index += 1\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                if batch_size > 1:\n",
    "                    print(\"Reducing batch size and retrying...\")\n",
    "                    return get_matrix_dataset_embeddings(dataset, model_path, fasta_ids, batch_size=batch_size // 2, device=device)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################save as .pkl file#################\n",
    "import pickle\n",
    "save_path = '/root/autodl-tmp/TaxoChallenge/embeddings_with_phylum_43587_matrix.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(matrix_embeddings, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
